
# Application
server.baseurl=http://kp2s.com
server.servlet.context-path=/api/

# Kafka
spring.config.import=optional:zookeeper:
spring.kafka.consumer.bootstrap-servers=localhost:9092
spring.kafka.consumer.group-id=group_id
spring.kafka.consumer.auto-offset-reset=earliest
spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
spring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer
spring.kafka.producer.bootstrap-servers=localhost:9092
spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer
spring.kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerializer

# Elasticsearch
spring.data.elasticsearch.client.reactive.endpoints=192.168.99.100:9200
spring.elasticsearch.uris=http://192.168.99.100:9200

# Logger
logging.level.org.springframework.web=debug
logging.level.org.hibernate=error
logging.level.com.kp2s=error
logging.file.name=${java.io.tmpdir}/log/sakapfet/app.log
# Log Patterns
logging.pattern.console= %d{yyyy-MM-dd HH:mm:ss} - %msg%n
logging.pattern.file= %d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level %logger{36} - %msg%
#The filename pattern used to create log archives.
#logging.logback.rollingpolicy.file-name-pattern=/tmp/log/sakapfet/%d{yyyy-MM, aux}/app.%d{yyyy-MM-dd}.%i.log
#The maximum size of log file before it is archived.
#logging.logback.rollingpolicy.max-file-size=100MB
#The maximum amount of size log archives can take before being deleted.
#logging.logback.rollingpolicy.total-size-cap=10GB
#The maximum number of archive log files to keep (defaults to 7).
#logging.logback.rollingpolicy.max-history=10

# Cloud
spring.cloud.zookeeper.connect-string=localhost:22181

# Spring profile
spring.profiles.active=dev

# R2DBC Postgresql
spring.r2dbc.url=r2dbc:postgresql://localhost:5432/develop
spring.r2dbc.username=sakapfet
spring.r2dbc.password=sak@pfet
spring.r2dbc.pool.initialSize=1
spring.r2dbc.pool.maxSize=2
spring.r2dbc.pool.max-idle-time=3m
